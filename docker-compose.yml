
services:

  llm:
    image: ollama/ollama:latest
    networks:
      - net
    environment:
      - OLLAMA_BASE_URL=http://localhost:11434
    ports:
      - 11434:11434

  pull-model:
    image: genai-stack/pull-model:latest
    build:
      context: .
      dockerfile: pull_model.Dockerfile
    env_file:
      - .env
    environment:
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL}
      - LLM=${LLM}
    networks:
      - net
    tty: true
    depends_on:
      - llm

  api:
    build:
      context: .
      dockerfile: api.Dockerfile
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}  
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL}
      - LLM=${LLM}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL}
      - LANGCHAIN_API_KEY=${LANGCHAIN_API_KEY}
    networks:
      - net
    depends_on:
      pull-model:
        condition: service_completed_successfully
    x-develop:
      watch:
        - action: rebuild
          path: .
          ignore:
            - loader.py
            - bot.py
    ports:
      - 8504:8504
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 http://localhost:8504/ || exit 1"]
      interval: 15s
      timeout: 30s
      retries: 5

  telegram_bot:
    build:
      context: ./telegram_bot
      dockerfile: telegram_bot.Dockerfile
    env_file:
      - .env
    environment:
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL}
      - LLM=${LLM}
    networks:
      - net
    ports:
      - 8506:8506
    healthcheck:
      test: ["CMD-SHELL", "curl --fail http://localhost:8506/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    depends_on:
      pull-model:
        condition: service_completed_successfully

networks:
  net:
    name: net
    driver: bridge
